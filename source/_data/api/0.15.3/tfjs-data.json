{
  "docs": {
    "headings": [
      {
        "name": "Data",
        "description": "",
        "subheadings": [
          {
            "name": "Classes",
            "symbols": [
              {
                "docInfo": {
                  "heading": "Data",
                  "subheading": "Classes",
                  "namespace": "data"
                },
                "symbolName": "Dataset",
                "namespace": "data",
                "documentation": "Represents a potentially large list of independent data elements (typically\n'samples' or 'examples').\n\nA 'data example' may be a primitive, an array, a map from string keys to\nvalues, or any nested structure of these.\n\nA `Dataset` represents an ordered collection of elements, together with a\nchain of transformations to be performed on those elements. Each\ntransformation is a method of `Dataset` that returns another `Dataset`, so\nthese may be chained, e.g.\n`const processedDataset = rawDataset.filter(...).map(...).batch(...)`.\n\nData loading and transformation is done in a lazy, streaming fashion.  The\ndataset may be iterated over multiple times; each iteration starts the data\nloading anew and recapitulates the transformations.\n\nA `Dataset` is typically processed as a stream of unbatched examples --i.e.,\nits transformations are applied one example at a time. Batching produces a\nnew `Dataset` where each element is a batch. Batching should usually come\nlast in a pipeline, because data transformations are easier to express on a\nper-example basis than on a per-batch basis.\n\nThe following code examples are calling `await dataset.forEachAsync(...)` to\niterate once over the entire dataset in order to print out the data.",
                "fileName": "#54",
                "githubUrl": "https://github.com/tensorflow/tfjs-data/blob/v0.2.3/src/dataset.ts#L54-L477",
                "methods": [
                  {
                    "docInfo": {
                      "heading": "Data",
                      "subheading": "Classes"
                    },
                    "symbolName": "batch",
                    "paramStr": "(batchSize, smallLastBatch?)",
                    "parameters": [
                      {
                        "name": "batchSize",
                        "documentation": "The number of elements desired per batch.",
                        "type": "number",
                        "optional": false,
                        "isConfigParam": false
                      },
                      {
                        "name": "smallLastBatch",
                        "documentation": "Whether to emit the final batch when it has fewer\nthan batchSize elements. Default true.",
                        "type": "boolean",
                        "optional": true,
                        "isConfigParam": false
                      }
                    ],
                    "returnType": "Dataset",
                    "documentation": "Groups elements into batches and arranges their values in columnar\nform.\n\nIt is assumed that each of the incoming dataset elements has the same\nset of keys.  For each key, the resulting `Dataset` provides a batched\nelement collecting all of the incoming values for that key.  Incoming\nstrings are grouped into a string[].  Incoming Tensors are grouped into a\nnew Tensor where the 0'th axis is the batch dimension.  These columnar\nrepresentations for each key can be zipped together to reconstruct the\noriginal dataset elements.\n\nBatch a dataset of numbers:\n```js\nconst a = tf.data.array([1, 2, 3, 4, 5, 6, 7, 8]).batch(4);\nawait a.forEachAsync(e => e.print());\n```\n\nBatch a dataset of arrays:\n```js\nconst b = tf.data.array([[1], [2], [3], [4], [5], [6], [7], [8]]).batch(4);\nawait b.forEachAsync(e => e.print());\n```\n\nBatch a dataset of objects:\n```js\nconst c = tf.data.array([{a: 1, b: 11}, {a: 2, b: 12}, {a: 3, b: 13},\n   {a: 4, b: 14}, {a: 5, b: 15}, {a: 6, b: 16}, {a: 7, b: 17},\n   {a: 8, b: 18}]).batch(4);\nawait c.forEachAsync(e => {\n   console.log('{');\n   for(var key in e) {\n     console.log(key+':');\n     e[key].print();\n   }\n   console.log('}');\n})\n```",
                    "fileName": "#118",
                    "githubUrl": "https://github.com/tensorflow/tfjs-data/blob/v0.2.3/src/dataset.ts#L118-L140",
                    "isFunction": true
                  },
                  {
                    "docInfo": {
                      "heading": "Data",
                      "subheading": "Classes"
                    },
                    "symbolName": "concatenate",
                    "paramStr": "(dataset)",
                    "parameters": [
                      {
                        "name": "dataset",
                        "documentation": "A `Dataset` to be concatenated onto this one.",
                        "type": "Dataset",
                        "optional": false,
                        "isConfigParam": false
                      }
                    ],
                    "returnType": "Dataset",
                    "documentation": "Concatenates this `Dataset` with another.\n\n```js\nconst a = tf.data.array([1, 2, 3]);\nconst b = tf.data.array([4, 5, 6]);\nconst c = a.concatenate(b);\nawait c.forEachAsync(e => console.log(e));\n```",
                    "fileName": "#156",
                    "githubUrl": "https://github.com/tensorflow/tfjs-data/blob/v0.2.3/src/dataset.ts#L156-L176",
                    "isFunction": true
                  },
                  {
                    "docInfo": {
                      "heading": "Data",
                      "subheading": "Classes"
                    },
                    "symbolName": "filter",
                    "paramStr": "(predicate)",
                    "parameters": [
                      {
                        "name": "predicate",
                        "documentation": "A function mapping a dataset element to a boolean or a\n`Promise` for one.",
                        "type": "(value: T) => boolean",
                        "optional": false,
                        "isConfigParam": false
                      }
                    ],
                    "returnType": "Dataset",
                    "documentation": "Filters this dataset according to `predicate`.\n\n```js\nconst a = tf.data.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n   .filter(x => x%2 === 0);\nawait a.forEachAsync(e => console.log(e));\n```",
                    "fileName": "#193",
                    "githubUrl": "https://github.com/tensorflow/tfjs-data/blob/v0.2.3/src/dataset.ts#L193-L207",
                    "isFunction": true
                  },
                  {
                    "docInfo": {
                      "heading": "Data",
                      "subheading": "Classes"
                    },
                    "symbolName": "forEachAsync",
                    "paramStr": "(f)",
                    "parameters": [
                      {
                        "name": "f",
                        "documentation": "A function to apply to each dataset element.",
                        "type": "(input: T) => void",
                        "optional": false,
                        "isConfigParam": false
                      }
                    ],
                    "returnType": "Promise",
                    "documentation": "Apply a function to every element of the dataset.\n\nAfter the function is applied to a dataset element, any Tensors contained\nwithin that element are disposed.\n\n```js\nconst a = tf.data.array([1, 2, 3]);\nawait a.forEachAsync(e => console.log(e));\n```",
                    "fileName": "#224",
                    "githubUrl": "https://github.com/tensorflow/tfjs-data/blob/v0.2.3/src/dataset.ts#L224-L226",
                    "isFunction": true
                  },
                  {
                    "docInfo": {
                      "heading": "Data",
                      "subheading": "Classes"
                    },
                    "symbolName": "map",
                    "paramStr": "(transform)",
                    "parameters": [
                      {
                        "name": "transform",
                        "documentation": "A function mapping a dataset element to a transformed\ndataset element.",
                        "type": "(value: T) => DataElement",
                        "optional": false,
                        "isConfigParam": false
                      }
                    ],
                    "returnType": "Dataset",
                    "documentation": "Maps this dataset through a 1-to-1 transform.\n\n```js\nconst a = tf.data.array([1, 2, 3]).map(x => x*x);\nawait a.forEachAsync(e => console.log(e));\n```",
                    "fileName": "#250",
                    "githubUrl": "https://github.com/tensorflow/tfjs-data/blob/v0.2.3/src/dataset.ts#L250-L255",
                    "isFunction": true
                  },
                  {
                    "docInfo": {
                      "heading": "Data",
                      "subheading": "Classes"
                    },
                    "symbolName": "mapAsync",
                    "paramStr": "(transform)",
                    "parameters": [
                      {
                        "name": "transform",
                        "documentation": "A function mapping a dataset element to a `Promise` for a\ntransformed dataset element.  This transform is responsible for disposing\nany intermediate `Tensor`s, i.e. by wrapping its computation in\n`tf.tidy()`; that cannot be automated here (as it is in the synchronous\n`map()` case).",
                        "type": "(value: T) => Promise",
                        "optional": false,
                        "isConfigParam": false
                      }
                    ],
                    "returnType": "Dataset",
                    "documentation": "Maps this dataset through an async 1-to-1 transform.\n\n```js\nconst a = tf.data.array([1, 2, 3]).map(x => new Promise(function(resolve){\n  resolve(x*x);\n}));\nawait a.forEachAsync(e => e.then(function(value){\n  console.log(value);\n}));\n```",
                    "fileName": "#278",
                    "githubUrl": "https://github.com/tensorflow/tfjs-data/blob/v0.2.3/src/dataset.ts#L278-L284",
                    "isFunction": true
                  },
                  {
                    "docInfo": {
                      "heading": "Data",
                      "subheading": "Classes"
                    },
                    "symbolName": "prefetch",
                    "paramStr": "(bufferSize)",
                    "parameters": [
                      {
                        "name": "bufferSize",
                        "documentation": ": An integer specifying the number of elements to be\nprefetched.",
                        "type": "number",
                        "optional": false,
                        "isConfigParam": false
                      }
                    ],
                    "returnType": "Dataset",
                    "documentation": "Creates a `Dataset` that prefetches elements from this dataset.",
                    "fileName": "#295",
                    "githubUrl": "https://github.com/tensorflow/tfjs-data/blob/v0.2.3/src/dataset.ts#L295-L299",
                    "isFunction": true
                  },
                  {
                    "docInfo": {
                      "heading": "Data",
                      "subheading": "Classes"
                    },
                    "symbolName": "repeat",
                    "paramStr": "(count?)",
                    "parameters": [
                      {
                        "name": "count",
                        "documentation": ": (Optional) An integer, representing the number of times\nthe dataset should be repeated. The default behavior (if `count` is\n`undefined` or negative) is for the dataset be repeated indefinitely.",
                        "type": "number",
                        "optional": true,
                        "isConfigParam": false
                      }
                    ],
                    "returnType": "Dataset",
                    "documentation": "Repeats this dataset `count` times.\n\nNOTE: If this dataset is a function of global state (e.g. a random number\ngenerator), then different repetitions may produce different elements.\n\n```js\nconst a = tf.data.array([1, 2, 3]).repeat(3);\nawait a.forEachAsync(e => console.log(e));\n```",
                    "fileName": "#318",
                    "githubUrl": "https://github.com/tensorflow/tfjs-data/blob/v0.2.3/src/dataset.ts#L318-L342",
                    "isFunction": true
                  },
                  {
                    "docInfo": {
                      "heading": "Data",
                      "subheading": "Classes"
                    },
                    "symbolName": "skip",
                    "paramStr": "(count)",
                    "parameters": [
                      {
                        "name": "count",
                        "documentation": ": The number of elements of this dataset that should be skipped\nto form the new dataset.  If `count` is greater than the size of this\ndataset, the new dataset will contain no elements.  If `count`\nis `undefined` or negative, skips the entire dataset.",
                        "type": "number",
                        "optional": false,
                        "isConfigParam": false
                      }
                    ],
                    "returnType": "Dataset",
                    "documentation": "Creates a `Dataset` that skips `count` initial elements from this dataset.\n\n```js\nconst a = tf.data.array([1, 2, 3, 4, 5, 6]).skip(3);\nawait a.forEachAsync(e => console.log(e));\n```",
                    "fileName": "#360",
                    "githubUrl": "https://github.com/tensorflow/tfjs-data/blob/v0.2.3/src/dataset.ts#L360-L380",
                    "isFunction": true
                  },
                  {
                    "docInfo": {
                      "heading": "Data",
                      "subheading": "Classes"
                    },
                    "symbolName": "shuffle",
                    "paramStr": "(bufferSize, seed?, reshuffleEachIteration?)",
                    "parameters": [
                      {
                        "name": "bufferSize",
                        "documentation": ": An integer specifying the number of elements from this\ndataset from which the new dataset will sample.",
                        "type": "number",
                        "optional": false,
                        "isConfigParam": false
                      },
                      {
                        "name": "seed",
                        "documentation": ": (Optional) An integer specifying the random seed that will\nbe used to create the distribution.",
                        "type": "string",
                        "optional": true,
                        "isConfigParam": false
                      },
                      {
                        "name": "reshuffleEachIteration",
                        "documentation": ": (Optional) A boolean, which if true\nindicates that the dataset should be pseudorandomly reshuffled each time\nit is iterated over. If false, elements will be returned in the same\nshuffled order on each iteration. (Defaults to `true`.)",
                        "type": "boolean",
                        "optional": true,
                        "isConfigParam": false
                      }
                    ],
                    "returnType": "Dataset",
                    "documentation": "Pseudorandomly shuffles the elements of this dataset. This is done in a\nstreaming manner, by sampling from a given number of prefetched elements.\n\n```js\nconst a = tf.data.array([1, 2, 3, 4, 5, 6]).shuffle(3);\nawait a.forEachAsync(e => console.log(e));\n```",
                    "fileName": "#404",
                    "githubUrl": "https://github.com/tensorflow/tfjs-data/blob/v0.2.3/src/dataset.ts#L404-L415",
                    "isFunction": true
                  },
                  {
                    "docInfo": {
                      "heading": "Data",
                      "subheading": "Classes"
                    },
                    "symbolName": "take",
                    "paramStr": "(count)",
                    "parameters": [
                      {
                        "name": "count",
                        "documentation": ": The number of elements of this dataset that should be taken\nto form the new dataset.  If `count` is `undefined` or negative, or if\n`count` is greater than the size of this dataset, the new dataset will\ncontain all elements of this dataset.",
                        "type": "number",
                        "optional": false,
                        "isConfigParam": false
                      }
                    ],
                    "returnType": "Dataset",
                    "documentation": "Creates a `Dataset` with at most `count` initial elements from this\ndataset.\n\n```js\nconst a = tf.data.array([1, 2, 3, 4, 5, 6]).take(3);\nawait a.forEachAsync(e => console.log(e));\n```",
                    "fileName": "#433",
                    "githubUrl": "https://github.com/tensorflow/tfjs-data/blob/v0.2.3/src/dataset.ts#L433-L450",
                    "isFunction": true
                  },
                  {
                    "docInfo": {
                      "heading": "Data",
                      "subheading": "Classes"
                    },
                    "symbolName": "toArray",
                    "paramStr": "()",
                    "parameters": [],
                    "returnType": "{}",
                    "documentation": "Collect all elements of this dataset into an array.\n\nObviously this will succeed only for small datasets that fit in memory.\nUseful for testing and generally should be avoided if possible.\n\n```js\nconst a = tf.data.array([1, 2, 3, 4, 5, 6]);\nconsole.log(await a.toArray());\n```",
                    "fileName": "#467",
                    "githubUrl": "https://github.com/tensorflow/tfjs-data/blob/v0.2.3/src/dataset.ts#L467-L469",
                    "isFunction": true
                  }
                ],
                "isClass": true
              },
              {
                "docInfo": {
                  "heading": "Data",
                  "subheading": "Classes",
                  "namespace": "data"
                },
                "symbolName": "CSVDataset",
                "namespace": "data",
                "documentation": "Represents a potentially large collection of delimited text records.\n\nThe produced `DataElement`s each contain one key-value pair for\nevery column of the table.  When a field is empty in the incoming data, the\nresulting value is `undefined`, or throw error if it is required.  Values\nthat can be parsed as numbers are emitted as type `number`, other values\nare parsed as `string`.\n\nThe results are not batched.",
                "fileName": "#45",
                "githubUrl": "https://github.com/tensorflow/tfjs-data/blob/v0.2.3/src/datasets/csv_dataset.ts#L45-L369",
                "methods": [
                  {
                    "docInfo": {
                      "heading": "Data",
                      "subheading": "Classes"
                    },
                    "symbolName": "columnNames",
                    "paramStr": "()",
                    "parameters": [],
                    "returnType": "{}",
                    "documentation": "Returns column names of the csv dataset. If `configuredColumnsOnly` is\ntrue, return column names in `columnConfigs`. If `configuredColumnsOnly` is\nfalse and `columnNames` is provided, `columnNames`. If\n`configuredColumnsOnly` is false and `columnNames` is not provided, return\nall column names parsed from the csv file. For example usage please go to\n`tf.data.csv`.",
                    "fileName": "#63",
                    "githubUrl": "https://github.com/tensorflow/tfjs-data/blob/v0.2.3/src/datasets/csv_dataset.ts#L63-L69",
                    "isFunction": true
                  }
                ],
                "isClass": true,
                "inheritsFrom": "Dataset"
              }
            ]
          },
          {
            "name": "Creation",
            "symbols": [
              {
                "docInfo": {
                  "heading": "Data",
                  "subheading": "Creation",
                  "namespace": "data"
                },
                "symbolName": "array",
                "namespace": "data",
                "paramStr": "(items)",
                "parameters": [
                  {
                    "name": "items",
                    "documentation": "An array of elements that will be parsed as items in a dataset.",
                    "type": "DataElement[]",
                    "optional": false,
                    "isConfigParam": false
                  }
                ],
                "returnType": "Dataset",
                "documentation": "Create a `Dataset` from an array of elements.\n\nCreate a Dataset from an array of objects:\n```js\nconst a = tf.data.array([{'item': 1}, {'item': 2}, {'item': 3}]);\nawait a.forEachAsync(e => console.log(e));\n```\n\nCreate a Dataset from an array of numbers:\n```js\nconst a = tf.data.array([4, 5, 6]);\nawait a.forEachAsync(e => console.log(e));\n```",
                "fileName": "#525",
                "githubUrl": "https://github.com/tensorflow/tfjs-data/blob/v0.2.3/src/dataset.ts#L525-L528",
                "isFunction": true
              },
              {
                "docInfo": {
                  "heading": "Data",
                  "subheading": "Creation",
                  "namespace": "data",
                  "configParamIndices": [
                    1
                  ]
                },
                "symbolName": "csv",
                "namespace": "data",
                "paramStr": "(source, csvConfig?)",
                "parameters": [
                  {
                    "name": "source",
                    "documentation": "URL or local path to get CSV file. If it's a local path, it\nmust have prefix `file://` and it only works in node environment.",
                    "type": "string",
                    "optional": false,
                    "isConfigParam": false
                  },
                  {
                    "name": "csvConfig",
                    "documentation": "(Optional) A CSVConfig object that contains configurations\nof reading and decoding from CSV file(s).",
                    "type": "CSVConfig",
                    "optional": true,
                    "isConfigParam": false
                  }
                ],
                "returnType": "CSVDataset",
                "documentation": "Create a `CSVDataset` by reading and decoding CSV file(s) from provided URL\nor local path if it's in Node environment.\n\n```js\nconst csvUrl =\n'https://storage.googleapis.com/tfjs-examples/multivariate-linear-regression/data/boston-housing-train.csv';\n\nasync function run() {\n   // We want to predict the column \"medv\", which represents a median value of\n   // a home (in $1000s), so we mark it as a label.\n   const csvDataset = tf.data.csv(\n     csvUrl, {\n       columnConfigs: {\n         medv: {\n           isLabel: true\n         }\n       }\n     });\n\n   // Number of features is the number of column names minus one for the label\n   // column.\n   const numOfFeatures = (await csvDataset.columnNames()).length - 1;\n\n   // Prepare the Dataset for training.\n   const flattenedDataset =\n     csvDataset\n     .map(([rawFeatures, rawLabel]) =>\n       // Convert rows from object form (keyed by column name) to array form.\n       [Object.values(rawFeatures), Object.values(rawLabel)])\n     .batch(10);\n\n   // Define the model.\n   const model = tf.sequential();\n   model.add(tf.layers.dense({\n     inputShape: [numOfFeatures],\n     units: 1\n   }));\n   model.compile({\n     optimizer: tf.train.sgd(0.000001),\n     loss: 'meanSquaredError'\n   });\n\n   // Fit the model using the prepared Dataset\n   return model.fitDataset(flattenedDataset, {\n     epochs: 10,\n     callbacks: {\n       onEpochEnd: async (epoch, logs) => {\n         console.log(epoch + ':' + logs.loss);\n       }\n     }\n   });\n}\n\nawait run();\n```",
                "fileName": "#95",
                "githubUrl": "https://github.com/tensorflow/tfjs-data/blob/v0.2.3/src/readers.ts#L95-L97",
                "isFunction": true
              },
              {
                "docInfo": {
                  "heading": "Data",
                  "subheading": "Creation",
                  "namespace": "data",
                  "configParamIndices": [
                    1
                  ]
                },
                "symbolName": "generator",
                "namespace": "data",
                "paramStr": "(f)",
                "parameters": [
                  {
                    "name": "f",
                    "documentation": "A function that produces one data element on each call.",
                    "type": "() => IteratorResult",
                    "optional": false,
                    "isConfigParam": false
                  }
                ],
                "returnType": "Dataset",
                "documentation": "Create a `Dataset` that produces each element by calling a provided function.\n\nNote that repeated iterations over this `Dataset` may produce different\nresults, because the function will be called anew for each element of each\niteration.\n\nAlso, beware that the sequence of calls to this function may be out of order\nin time with respect to the logical order of the Dataset. This is due to the\nasynchronous lazy nature of stream processing, and depends on downstream\ntransformations (e.g. .shuffle()). If the provided function is pure, this is\nno problem, but if it is a closure over a mutable state (e.g., a traversal\npointer), then the order of the produced elements may be scrambled.\n\n```js\nlet i = -1;\nconst func = () =>\n    ++i < 5 ? {value: i, done: false} : {value: null, done: true};\nconst ds = tf.data.generator(func);\nawait ds.forEachAsync(e => console.log(e));\n```",
                "fileName": "#132",
                "githubUrl": "https://github.com/tensorflow/tfjs-data/blob/v0.2.3/src/readers.ts#L132-L136",
                "isFunction": true
              }
            ]
          },
          {
            "name": "Operations",
            "symbols": [
              {
                "docInfo": {
                  "heading": "Data",
                  "subheading": "Operations",
                  "namespace": "data"
                },
                "symbolName": "zip",
                "namespace": "data",
                "paramStr": "(datasets)",
                "parameters": [
                  {
                    "name": "datasets",
                    "documentation": "",
                    "type": "DatasetContainer",
                    "optional": false,
                    "isConfigParam": false
                  }
                ],
                "returnType": "Dataset",
                "documentation": "Create a `Dataset` by zipping together an array, dict, or nested\nstructure of `Dataset`s (and perhaps additional constants).\nThe underlying datasets must provide elements in a consistent order such that\nthey correspond.\n\nThe number of elements in the resulting dataset is the same as the size of\nthe smallest dataset in datasets.\n\nThe nested structure of the `datasets` argument determines the\nstructure of elements in the resulting iterator.\n\nNote this means that, given an array of two datasets that produce dict\nelements, the result is a dataset that produces elements that are arrays\nof two dicts:\n\nZip an array of datasets:\n```js\nconsole.log('Zip two datasets of objects:');\nconst ds1 = tf.data.array([{a: 1}, {a: 2}, {a: 3}]);\nconst ds2 = tf.data.array([{b: 4}, {b: 5}, {b: 6}]);\nconst ds3 = tf.data.zip([ds1, ds2]);\nawait ds3.forEachAsync(e => console.log(JSON.stringify(e)));\n\n// If the goal is to merge the dicts in order to produce elements like\n// {a: ..., b: ...}, this requires a second step such as:\nconsole.log('Merge the objects:');\nconst ds4 = ds3.map(x => {return {a: x[0].a, b: x[1].b}});\nawait ds4.forEachAsync(e => console.log(e));\n```\n\nZip a dict of datasets:\n```js\nconst a = tf.data.array([{a: 1}, {a: 2}, {a: 3}]);\nconst b = tf.data.array([{b: 4}, {b: 5}, {b: 6}]);\nconst c = tf.data.zip({c: a, d: b});\nawait c.forEachAsync(e => console.log(JSON.stringify(e)));\n```",
                "fileName": "#570",
                "githubUrl": "https://github.com/tensorflow/tfjs-data/blob/v0.2.3/src/dataset.ts#L570-L602",
                "isFunction": true
              }
            ]
          }
        ]
      }
    ]
  },
  "docLinkAliases": {},
  "configInterfaceParamMap": {
    "ContainerObject": [],
    "ContainerArray": [],
    "ColumnConfig": [
      {
        "name": "required",
        "documentation": "",
        "type": "boolean",
        "optional": true,
        "isConfigParam": true
      },
      {
        "name": "dtype",
        "documentation": "",
        "type": "DataType",
        "optional": true,
        "isConfigParam": true
      },
      {
        "name": "default",
        "documentation": "",
        "type": "DataElement",
        "optional": true,
        "isConfigParam": true
      },
      {
        "name": "isLabel",
        "documentation": "",
        "type": "boolean",
        "optional": true,
        "isConfigParam": true
      }
    ],
    "CSVConfig": [
      {
        "name": "hasHeader",
        "documentation": "A boolean value that indicates whether the first row of provided CSV file\nis a header line with column names, and should not be included in the data.",
        "type": "boolean",
        "optional": true,
        "isConfigParam": true
      },
      {
        "name": "columnNames",
        "documentation": "A list of strings that corresponds to the CSV column names, in order. If\nprovided, it ignores the column names inferred from the header row. If not\nprovided, infers the column names from the first row of the records. If\n`hasHeader` is false and `columnNames` is not provided, this method will\nthrow an error.",
        "type": "string[]",
        "optional": true,
        "isConfigParam": true
      },
      {
        "name": "columnConfigs",
        "documentation": "A dictionary whose key is column names, value is an object stating if this\ncolumn is required, column's data type, default value, and if this column\nis label. If provided, keys must correspond to names provided in\n`columnNames` or inferred from the file header lines. If any column is\nmarked as label, the .csv() API will return an array of two items: the\nfirst item is a dict of features key/value pairs, the second item is a dict\nof labels key/value pairs. If no column is marked as label returns a dict\nof features only.\n\nHas the following fields:\n- `required` If value in this column is required. If set to `true`, throw\nan error when it finds an empty value.\n\n- `dtype` Data type of this column. Could be int32, float32, bool, or\nstring.\n\n- `default` Default value of this column.\n\n- `isLabel` Whether this column is label instead of features. If isLabel is\n`true` for at least one column, the .csv() API will return an array of two\nitems: the first item is a dict of features key/value pairs, the second\nitem is a dict of labels key/value pairs. If no column is marked as label\nreturns a dict of features only.",
        "type": "{[key: string]: ColumnConfig}",
        "optional": true,
        "isConfigParam": true
      },
      {
        "name": "configuredColumnsOnly",
        "documentation": "If true, only columns provided in `columnConfigs` will be parsed and\nprovided during iteration.",
        "type": "boolean",
        "optional": true,
        "isConfigParam": true
      },
      {
        "name": "delimiter",
        "documentation": "The string used to parse each line of the input file.",
        "type": "string",
        "optional": true,
        "isConfigParam": true
      }
    ],
    "FileChunkIteratorOptions": [
      {
        "name": "offset",
        "documentation": "The byte offset at which to begin reading the File or Blob. Default 0. ",
        "type": "number",
        "optional": true,
        "isConfigParam": true
      },
      {
        "name": "chunkSize",
        "documentation": "The number of bytes to read at a time. Default 1MB. ",
        "type": "number",
        "optional": true,
        "isConfigParam": true
      }
    ]
  },
  "inlineTypes": {},
  "docTypeAliases": {}
}