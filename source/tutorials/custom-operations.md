---
title: custom-operation
date: 2018-08-02 16:07:20
---

### Sections
---
* [Creating custom WebGL operations](#tutorial)
* [GLSL functions provided by Tensorflow.js](#stdlib)


# <a id="tutorial"></a>Creating custom WebGL operations

To define custom WebGL operations, all we have to do is create an object that implements `tf.webgl.GPGPUProgram`.

This interface is defined as:
```ts
interface GPGPUProgram {
  variableNames: string[];
  outputShape: number[];
  userCode: string;
  supportsBroadcasting?: boolean;
}
```

For a contrived example, lets implement an operation that computes `f(x) = x * x + x`.

The GLSL code for this would be:
```glsl
void main() {
    float x = getXAtOutCoords();
    float value = x * x + x;
    setOutput(value);
}
```

where `getXAtOutCoords` and `setOutput` are [provided by Tensorflow.js](#stdlib) to the shader.

The full GPGPUProgram definition would be:
```js
const squareAndAddKernel = inputShape => ({
  variableNames: ['X'],
  outputShape: inputShape.slice(),
  userCode: `
    void main() {
        float x = getXAtOutCoords();
        float value = x * x + x;
        setOutput(value);
      }
  `
})
```

To run this op, you would use `tf.ENV.backend.compileAndRun(GPGPUProgram, tf.Tensor[]): tf.Tensor`. Note that this will be undefined if the backend isn't the webgl backend.

```js
const x = tf.tensor([1, 2, 3, 4]);
const program = squareAndAddKernel(x.shape);

const result = tf.ENV.backend.compileAndRun(program, [x]);
```

However, we probably also want to define the gradients for this op, so that gradients can be backpropagated through it.

To do this, we use `tf.customGrad`.

```js
const squareAndAddBackpropKernel = inputShape => ({
  variableNames: ['X'],
  outputShape: inputShape.slice(),
  userCode: `
    void main() {
      float x = getXAtOutCoords();
      float value = 2.0 * x + 1.0;
      setOutput(value);
    }
  `
});


const squareAndAdd = tf.customGrad(x => {
  const backend = tf.ENV.backend;
  const program = squareAndAddKernel(x.shape);
  const backpropProgram = squareAndAddBackpropKernel(x.shape);

  const value = backend.compileAndRun(program, [x]);

  const gradFunc = dy =>
      [backend.compileAndRun(backpropProgram, [x]).mul(dy)];
  return {value, gradFunc}
});
```

We can then use this as:

```js
const x = tf.tensor([1, 2, 3, 4]);

const value = squareAndAdd(x);

const grads = tf.grad(x => squareAndAdd(x);
const grad = grads(input);

// value == [3, 6, 12, 20]
// grad == [3, 5, 7, 9]
```

Or more concisely:

```js
const {value, grad} = tf.valueAndGrad(squareAndAdd)(x);
```

# <a id="stdlib"></a>GLSL functions generated by Tensorflow.js

Tensorflow.js generates functions you can use to read from the input tensors and write to the output tensor. These are prepended to your code by the [Shader Compiler](https://github.com/tensorflow/tfjs-core/blob/master/src/kernels/webgl/shader_compiler.ts).

* `void setOutput(float value)`

  * In WebGL, there are no random writes, so we can only set the output for a single cell (equivalent to `gl_FragCoord = vec4(value, 0.0, 0.0, 0.0)`).

* `indexType getOutputCoords()`

  * Where `indexType` is one of `int | ivec2 | ivec3 | ivec4 | ivec5 | ivec6`.

  * Returns an `int` if the output tensor is rank-0 or rank-1, otherwise returns an `ivecN` where N == rank. This is the coordinate of the cell in the output tensor this thread will write to.


* Tensorflow.js generates GLSL functions to sample from the input tensors. These are of the form:

  ```glsl
    float get{VarName}AtOutCoords()

    float get{VarName}() // rank-0 input
    float get{VarName}(int x) // rank-1 input
    float get{VarName}(int x, int y) // rank-2 input
    float get{VarName}(int x, int y, int z) // rank-3 input
    float get{VarName}(int x, int y, int z, int w) // rank-4 input
    // continue as above for rank-5 & rank-6
  ```

  Where `VarName` is a variable name as defined in the `variableNames` array of your `GPGPUProgram` in **with the first letter captialised**.
  This means that for a variable named `ecks`, TF.js will generate `getEcks`.

  Many of these functions are depended on the rank of the input tensors, so in your `GPGPUProgram` you'll often want to emit different code based on the ranks of the `inputShape`s.
  For instance, if `get{VarName}AtOutCoords()` didn't exist, we might have written `squareAndAddKernel` as:,

  ```js
  const squareAndAddKernel = inputShape => ({
    const variableNames = ['X']
    const outputShape = inputShape.slice()
    const rank = outputShape.length

    const coordSnippets = ['',
        'coords',
        'coords.x, coords.y',
        'coords.x, coords.y, coords.z',
        'coords.x, coords.y, coords.z, coords.w']

    const coordType = rank < 2 ? 'int' : `ivec${rank}`

    const userCode = `
      void main() {
        ${coordType} coords = getOutputCoords();
        float x = getX(${coordSnippets[rank]});
        setOutput(x * x + x);
      }`

    return {variableNames, outputShape, userCode}
  })
  ```

  * `bool isNaN(float val)`

    * `true` if val is a `NaN`, otherwise false.

  * `int round(float value)`

    * Round `value` to the nearest integer.

  * `int imod(int x, int y)`

    * Same as `float mod(float x, float y)` but for ints, since GLSL doesn't provide us one.

  * `float random(float seed)`

    * Returns a pseudo-random number, based on Dav Hoskins's forumla in https://www.shadertoy.com/view/4djSRW.

